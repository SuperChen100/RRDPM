{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41ee607-b02d-4c8b-9886-cc6b47e1aa02",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chen/anaconda3/envs/paddle/lib/python3.10/site-packages/paddle/utils/cpp_extension/extension_utils.py:711: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "from paddle.io import Dataset, DataLoader  \n",
    "from PIL import Image\n",
    "import paddle\n",
    "import paddle as P\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "from paddle import ParamAttr\n",
    "from paddle.nn import Conv2D, BatchNorm, Linear, Dropout, AdaptiveAvgPool2D, MaxPool2D, AvgPool2D\n",
    "import pdb\n",
    "\n",
    "nn.initializer.set_global_initializer(nn.initializer.Normal(mean=0.0,std=0.01), nn.initializer.Constant())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7921c1f0-b0a5-455d-88d8-ff82aee70e15",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the training set\n",
    "class HRLRDataset(Dataset):  \n",
    "    def __init__(self):  \n",
    "        \"\"\"    \n",
    "        :param hr_dir: The folder path where the high-resolution images are located  \n",
    "        :param lr_dir: The folder path where the low-resolution image after interpolation is located\n",
    "        \"\"\"  \n",
    "        self.hr_dir = 'train/HR/'  \n",
    "        self.lr_dir = 'train/inter_LR/'  \n",
    "          \n",
    "        # Obtain the names of all images\n",
    "        self.image_names = [img_name for img_name in os.listdir(self.hr_dir) if img_name.endswith('.jpg')]  \n",
    "  \n",
    "    def __getitem__(self, idx):  \n",
    "        \"\"\"  \n",
    "        Obtain the HR and LR image pairs based on the index\n",
    "        :param idx: data index  \n",
    "        :return: HR and LR image pairs\n",
    "        \"\"\"  \n",
    "        img_name = self.image_names[idx]  \n",
    "          \n",
    "        # Load HR and LR images\n",
    "        hr_img_path = os.path.join(self.hr_dir, img_name)  \n",
    "        lr_img_path = os.path.join(self.lr_dir, img_name)  \n",
    "\n",
    "        \n",
    "        hr_img = Image.open(hr_img_path)\n",
    "        hr_img=np.array(hr_img, dtype=np.float32)/255\n",
    "        hr_img=hr_img*2-1\n",
    "        hr_img=hr_img[:,:,np.newaxis].transpose([2,0,1])\n",
    "\n",
    "        lr_img = Image.open(lr_img_path)\n",
    "        lr_img=np.array(lr_img, dtype=np.float32)/255\n",
    "        lr_img=lr_img*2-1\n",
    "        lr_img=lr_img[:,:,np.newaxis].transpose([2,0,1])\n",
    "\n",
    "        hr_img = paddle.to_tensor(hr_img)  \n",
    "        lr_img = paddle.to_tensor(lr_img)  \n",
    "          \n",
    "\n",
    "        return hr_img, lr_img  \n",
    "  \n",
    "    def __len__(self):  \n",
    "        \"\"\"  \n",
    "        Return the total number of samples in the dataset\n",
    "        \"\"\"  \n",
    "        return len(self.image_names)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a318d9b2-3568-4341-a90e-444b01f093bb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read the test set\n",
    "def read_test_dataset(n):\n",
    "    hr_dir = 'test/HR/'  \n",
    "    lr_dir = 'test/inter_LR/'  \n",
    "    lr0_dir = 'test/LR/'\n",
    "    images=os.listdir(hr_dir)\n",
    "    \n",
    "    HR0=np.zeros((n,1,120,120)).astype(\"float32\")\n",
    "    LR0=np.zeros((n,1,120,120)).astype(\"float32\")\n",
    "    LR00=np.zeros((n,1,25,25)).astype(\"float32\")\n",
    "    \n",
    "    for i in range(0,n):\n",
    "        img_name=random.choice(images)  \n",
    "        hr_img_path = os.path.join(hr_dir, img_name)  \n",
    "        lr_img_path = os.path.join(lr_dir, img_name)  \n",
    "        lr0_img_path = os.path.join(lr0_dir, img_name)  \n",
    "    \n",
    "        hr_img = Image.open(hr_img_path)\n",
    "        hr_img=np.array(hr_img, dtype=np.float32)/255\n",
    "        hr_img=hr_img[:,:,np.newaxis].transpose([2,0,1])\n",
    "        HR0[i,:,:,:]=hr_img\n",
    "    \n",
    "        lr_img = Image.open(lr_img_path)\n",
    "        lr_img=np.array(lr_img, dtype=np.float32)/255\n",
    "        lr_img=lr_img[:,:,np.newaxis].transpose([2,0,1])\n",
    "        LR0[i,:,:,:]=lr_img\n",
    "\n",
    "        lr0_img = Image.open(lr0_img_path)\n",
    "        lr0_img=np.array(lr0_img, dtype=np.float32)/255\n",
    "        lr0_img=lr0_img[:,:,np.newaxis].transpose([2,0,1])\n",
    "        LR00[i,:,:,:]=lr0_img\n",
    "\n",
    "    HR=paddle.to_tensor(HR0)\n",
    "    LR=paddle.to_tensor(LR0)\n",
    "    LR_0=paddle.to_tensor(LR00)\n",
    "    HR=HR*2-1\n",
    "    LR=LR*2-1\n",
    "    LR_0=LR_0*2-1\n",
    "    return HR,LR,LR_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a24d882-6620-4db1-b86f-fa03bf4c08db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# swin-transformer block\n",
    "import math\n",
    "\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "\n",
    "\n",
    "def to_2tuple(x):\n",
    "    if isinstance(x, int):\n",
    "        return (x, x)\n",
    "    else:\n",
    "        return tuple(x)\n",
    "\n",
    "\n",
    "class Mlp(nn.Layer):\n",
    "\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        x: (B, H, W, C)\n",
    "        window_size (int): window size\n",
    "    Returns:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "    \"\"\"\n",
    "    B, H, W, C = x.shape\n",
    "    x = x.reshape((B, H // window_size, window_size, W // window_size, window_size, C))\n",
    "    windows = x.transpose((0, 1, 3, 2, 4, 5)).reshape((-1, window_size, window_size, C))\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, H, W):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        windows: (num_windows*B, window_size, window_size, C)\n",
    "        window_size (int): Window size\n",
    "        H (int): Height of image\n",
    "        W (int): Width of image\n",
    "    Returns:\n",
    "        x: (B, H, W, C)\n",
    "    \"\"\"\n",
    "    B = int(windows.shape[0] / (H * W / window_size / window_size))\n",
    "    x = windows.reshape((B, H // window_size, W // window_size, window_size, window_size, -1))\n",
    "    x = x.transpose((0, 1, 3, 2, 4, 5)).reshape((B, H, W, -1))\n",
    "    return x\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Layer):\n",
    "    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n",
    "    It supports both of shifted and non-shifted window.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        window_size (tuple[int]): The height and width of the window.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n",
    "        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n",
    "        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size  # Wh, Ww\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "\n",
    "        # define a parameter table of relative position bias\n",
    "        self.relative_position_bias_table = self.create_parameter(shape=((2 * window_size[0] - 1) *\n",
    "                                                                         (2 * window_size[1] - 1), num_heads),\n",
    "                                                                  default_initializer=nn.initializer.Constant(0.0))\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        coords_h = paddle.arange(self.window_size[0])\n",
    "        coords_w = paddle.arange(self.window_size[1])\n",
    "        coords = paddle.stack(paddle.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "        coords_flatten = paddle.flatten(coords, 1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "        relative_coords = relative_coords.transpose((1, 2, 0))  # Wh*Ww, Wh*Ww, 2\n",
    "        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias_attr=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "        self.softmax = nn.Softmax(axis=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input features with shape of (num_windows*B, N, C)\n",
    "            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n",
    "        \"\"\"\n",
    "        B_, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape((B_, N, 3, self.num_heads, C // self.num_heads)).transpose((2, 0, 3, 1, 4))\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = (q @ k.transpose((0, 1, 3, 2)))\n",
    "\n",
    "        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.reshape(\n",
    "            (-1, ))].reshape((self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1],\n",
    "                              -1))  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.transpose((2, 0, 1))  # nH, Wh*Ww, Wh*Ww\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        if mask is not None:\n",
    "            nW = mask.shape[0]\n",
    "            attn = attn.reshape((B_ // nW, nW, self.num_heads, N, N)) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.reshape((-1, self.num_heads, N, N))\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose((0, 2, 1, 3)).reshape((B_, N, C))\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n",
    "\n",
    "    def flops(self, N):\n",
    "        # calculate flops for 1 window with token length of N\n",
    "        flops = 0\n",
    "        # qkv = self.qkv(x)\n",
    "        flops += N * self.dim * 3 * self.dim\n",
    "        # attn = (q @ k.transpose(-2, -1))\n",
    "        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n",
    "        #  x = (attn @ v)\n",
    "        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n",
    "        # x = self.proj(x)\n",
    "        flops += N * self.dim * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Layer):\n",
    "    r\"\"\" Swin Transformer Block.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resulotion.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Window size.\n",
    "        shift_size (int): Shift size for SW-MSA.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n",
    "        act_layer (nn.Layer, optional): Activation layer. Default: nn.GELU\n",
    "        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 num_heads,\n",
    "                 window_size=7,\n",
    "                 shift_size=0,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 act_layer=nn.GELU,\n",
    "                 norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        if min(self.input_resolution) <= self.window_size:\n",
    "            # if window size is larger than input resolution, we don't partition windows\n",
    "            self.shift_size = 0\n",
    "            self.window_size = min(self.input_resolution)\n",
    "        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(dim,\n",
    "                                    window_size=to_2tuple(self.window_size),\n",
    "                                    num_heads=num_heads,\n",
    "                                    qkv_bias=qkv_bias,\n",
    "                                    qk_scale=qk_scale,\n",
    "                                    attn_drop=attn_drop,\n",
    "                                    proj_drop=drop)\n",
    "\n",
    "        self.drop_path = nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if self.shift_size > 0:\n",
    "            attn_mask = self.calculate_mask(self.input_resolution)\n",
    "        else:\n",
    "            attn_mask = None\n",
    "\n",
    "        self.register_buffer(\"attn_mask\", attn_mask)\n",
    "\n",
    "    def calculate_mask(self, x_size):\n",
    "        # calculate attention mask for SW-MSA\n",
    "        H, W = x_size\n",
    "        img_mask = paddle.zeros((1, H, W, 1))  # 1 H W 1\n",
    "\n",
    "        h_slices = (slice(0, -self.window_size), slice(-self.window_size,\n",
    "                                                       -self.shift_size if self.shift_size else None),\n",
    "                    slice(-self.shift_size, None))\n",
    "        w_slices = (slice(0, -self.window_size), slice(-self.window_size,\n",
    "                                                       -self.shift_size if self.shift_size else None),\n",
    "                    slice(-self.shift_size, None))\n",
    "        cnt = 0\n",
    "        for h in h_slices:\n",
    "            for w in w_slices:\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n",
    "        mask_windows = mask_windows.reshape((-1, self.window_size * self.window_size))\n",
    "\n",
    "        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "        _h = paddle.full_like(attn_mask, -100.0, dtype='float32')\n",
    "        _z = paddle.full_like(attn_mask, 0.0, dtype='float32')\n",
    "        attn_mask = paddle.where(attn_mask != 0, _h, _z)\n",
    "\n",
    "        return attn_mask\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        H, W = x_size\n",
    "        B, L, C = x.shape\n",
    "        # assert L == H * W, \"input feature has wrong size\"\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = x.reshape((B, H, W, C))\n",
    "\n",
    "        # cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = paddle.roll(x, shifts=(-self.shift_size, -self.shift_size), axis=(1, 2))\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # partition windows\n",
    "        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n",
    "        x_windows = x_windows.reshape((-1, self.window_size * self.window_size, C))  # nW*B, window_size*window_size, C\n",
    "\n",
    "        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n",
    "        if self.input_resolution == x_size:\n",
    "            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n",
    "        else:\n",
    "            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size))\n",
    "\n",
    "        # merge windows\n",
    "        attn_windows = attn_windows.reshape((-1, self.window_size, self.window_size, C))\n",
    "        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n",
    "\n",
    "        # reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = paddle.roll(shifted_x, shifts=(self.shift_size, self.shift_size), axis=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "        x = x.reshape((B, H * W, C))\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n",
    "               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.input_resolution\n",
    "        # norm1\n",
    "        flops += self.dim * H * W\n",
    "        # W-MSA/SW-MSA\n",
    "        nW = H * W / self.window_size / self.window_size\n",
    "        flops += nW * self.attn.flops(self.window_size * self.window_size)\n",
    "        # mlp\n",
    "        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n",
    "        # norm2\n",
    "        flops += self.dim * H * W\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchMerging(nn.Layer):\n",
    "    r\"\"\" Patch Merging Layer.\n",
    "    Args:\n",
    "        input_resolution (tuple[int]): Resolution of input feature.\n",
    "        dim (int): Number of input channels.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.dim = dim\n",
    "        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "        self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B, H*W, C\n",
    "        \"\"\"\n",
    "        H, W = self.input_resolution\n",
    "        B, L, C = x.shape\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n",
    "\n",
    "        x = x.reshape((B, H, W, C))\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = paddle.concat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n",
    "        x = x.reshape((B, -1, 4 * C))  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.dim\n",
    "        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Layer):\n",
    "    \"\"\" A basic Swin Transformer layer for one stage.\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Layer | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "\n",
    "        # build blocks\n",
    "        self.blocks = nn.LayerList([\n",
    "            SwinTransformerBlock(dim=dim,\n",
    "                                 input_resolution=input_resolution,\n",
    "                                 num_heads=num_heads,\n",
    "                                 window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=mlp_ratio,\n",
    "                                 qkv_bias=qkv_bias,\n",
    "                                 qk_scale=qk_scale,\n",
    "                                 drop=drop,\n",
    "                                 attn_drop=attn_drop,\n",
    "                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                                 norm_layer=norm_layer) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        # patch merging layer\n",
    "        if downsample is not None:\n",
    "            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, x_size)\n",
    "        if self.downsample is not None:\n",
    "            x = self.downsample(x)\n",
    "        return x\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops()\n",
    "        if self.downsample is not None:\n",
    "            flops += self.downsample.flops()\n",
    "        return flops\n",
    "\n",
    "\n",
    "class RSTB(nn.Layer):\n",
    "    \"\"\"Residual Swin Transformer Block (RSTB).\n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        input_resolution (tuple[int]): Input resolution.\n",
    "        depth (int): Number of blocks.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        window_size (int): Local window size.\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n",
    "        drop (float, optional): Dropout rate. Default: 0.0\n",
    "        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n",
    "        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: nn.LayerNorm\n",
    "        downsample (nn.Layer | None, optional): Downsample layer at the end of the layer. Default: None\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n",
    "        img_size: Input image size.\n",
    "        patch_size: Patch size.\n",
    "        resi_connection: The convolutional block before residual connection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 dim,\n",
    "                 input_resolution,\n",
    "                 depth,\n",
    "                 num_heads,\n",
    "                 window_size,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop=0.,\n",
    "                 attn_drop=0.,\n",
    "                 drop_path=0.,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 downsample=None,\n",
    "                 use_checkpoint=False,\n",
    "                 img_size=224,\n",
    "                 patch_size=4,\n",
    "                 resi_connection='1conv'):\n",
    "        super(RSTB, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.input_resolution = input_resolution\n",
    "\n",
    "        self.residual_group = BasicLayer(dim=dim,\n",
    "                                         input_resolution=input_resolution,\n",
    "                                         depth=depth,\n",
    "                                         num_heads=num_heads,\n",
    "                                         window_size=window_size,\n",
    "                                         mlp_ratio=mlp_ratio,\n",
    "                                         qkv_bias=qkv_bias,\n",
    "                                         qk_scale=qk_scale,\n",
    "                                         drop=drop,\n",
    "                                         attn_drop=attn_drop,\n",
    "                                         drop_path=drop_path,\n",
    "                                         norm_layer=norm_layer,\n",
    "                                         downsample=downsample,\n",
    "                                         use_checkpoint=use_checkpoint)\n",
    "\n",
    "        if resi_connection == '1conv':\n",
    "            self.conv = nn.Conv2D(dim, dim, 3, 1, 1)\n",
    "        elif resi_connection == '3conv':\n",
    "            # to save parameters and memory\n",
    "            self.conv = nn.Sequential(nn.Conv2D(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2),\n",
    "                                      nn.Conv2D(dim // 4, dim // 4, 1, 1, 0), nn.LeakyReLU(negative_slope=0.2),\n",
    "                                      nn.Conv2D(dim // 4, dim, 3, 1, 1))\n",
    "\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size,\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=0,\n",
    "                                      embed_dim=dim,\n",
    "                                      norm_layer=None)\n",
    "\n",
    "        self.patch_unembed = PatchUnEmbed(img_size=img_size,\n",
    "                                          patch_size=patch_size,\n",
    "                                          in_chans=0,\n",
    "                                          embed_dim=dim,\n",
    "                                          norm_layer=None)\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.Silu(),\n",
    "            nn.Linear(\n",
    "                256,\n",
    "                dim\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        x_temp=self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))\n",
    "        # return self.patch_embed(x_temp) + x\n",
    "        return x_temp \n",
    "    \n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        flops += self.residual_group.flops()\n",
    "        H, W = self.input_resolution\n",
    "        flops += H * W * self.dim * self.dim * 9\n",
    "        flops += self.patch_embed.flops()\n",
    "        flops += self.patch_unembed.flops()\n",
    "\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Layer):\n",
    "    r\"\"\" Image to Patch Embedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        if norm_layer is not None:\n",
    "            self.norm = norm_layer(embed_dim)\n",
    "        else:\n",
    "            self.norm = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(2).transpose((0, 2, 1))  # B Ph*Pw C\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        H, W = self.img_size\n",
    "        if self.norm is not None:\n",
    "            flops += H * W * self.embed_dim\n",
    "        return flops\n",
    "\n",
    "\n",
    "class PatchUnEmbed(nn.Layer):\n",
    "    r\"\"\" Image to Patch Unembedding\n",
    "    Args:\n",
    "        img_size (int): Image size.  Default: 224.\n",
    "        patch_size (int): Patch token size. Default: 4.\n",
    "        in_chans (int): Number of input image channels. Default: 3.\n",
    "        embed_dim (int): Number of linear projection output channels. Default: 96.\n",
    "        norm_layer (nn.Layer, optional): Normalization layer. Default: None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patches_resolution = patches_resolution\n",
    "        self.num_patches = patches_resolution[0] * patches_resolution[1]\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def forward(self, x, x_size):\n",
    "        B, HW, C = x.shape\n",
    "        x = x.transpose((0, 2, 1)).reshape((B, self.embed_dim, x_size[0], x_size[1]))  # B Ph*Pw C\n",
    "        return x\n",
    "\n",
    "    def flops(self):\n",
    "        flops = 0\n",
    "        return flops\n",
    "\n",
    "\n",
    "class Upsample(nn.Sequential):\n",
    "    \"\"\"Upsample module.\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat):\n",
    "        m = []\n",
    "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
    "            for _ in range(int(math.log(scale, 2))):\n",
    "                m.append(nn.Conv2D(num_feat, 4 * num_feat, 3, 1, 1))\n",
    "                m.append(nn.PixelShuffle(2))\n",
    "        elif scale == 3:\n",
    "            m.append(nn.Conv2D(num_feat, 9 * num_feat, 3, 1, 1))\n",
    "            m.append(nn.PixelShuffle(3))\n",
    "        else:\n",
    "            raise ValueError(f'scale {scale} is not supported. '\n",
    "                             'Supported scales: 2^n and 3.')\n",
    "        super(Upsample, self).__init__(*m)\n",
    "\n",
    "\n",
    "class UpsampleOneStep(nn.Sequential):\n",
    "    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n",
    "       Used in lightweight SR to save parameters.\n",
    "    Args:\n",
    "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n",
    "        self.num_feat = num_feat\n",
    "        self.input_resolution = input_resolution\n",
    "        m = []\n",
    "        m.append(nn.Conv2D(num_feat, (scale**2) * num_out_ch, 3, 1, 1))\n",
    "        m.append(nn.PixelShuffle(scale))\n",
    "        super(UpsampleOneStep, self).__init__(*m)\n",
    "\n",
    "    def flops(self):\n",
    "        H, W = self.input_resolution\n",
    "        flops = H * W * self.num_feat * 3 * 9\n",
    "        return flops\n",
    "\n",
    "\n",
    "class Swin_Block(nn.Layer):\n",
    "    r\"\"\" SwinIR\n",
    "        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n",
    "    Args:\n",
    "        img_size (int | tuple(int)): Input image size. Default 64\n",
    "        patch_size (int | tuple(int)): Patch size. Default: 1\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        embed_dim (int): Patch embedding dimension. Default: 96\n",
    "        depths (tuple(int)): Depth of each Swin Transformer layer.\n",
    "        num_heads (tuple(int)): Number of attention heads in different layers.\n",
    "        window_size (int): Window size. Default: 7\n",
    "        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n",
    "        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n",
    "        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n",
    "        drop_rate (float): Dropout rate. Default: 0\n",
    "        attn_drop_rate (float): Attention dropout rate. Default: 0\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n",
    "        norm_layer (nn.Layer): Normalization layer. Default: nn.LayerNorm.\n",
    "        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n",
    "        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n",
    "        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n",
    "        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n",
    "        img_range: Image range. 1. or 255.\n",
    "        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n",
    "        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 img_size=119,\n",
    "                 patch_size=1,\n",
    "                 in_chans=1,\n",
    "                 embed_dim=96,\n",
    "                 depths=[6, 6, 6, 6],\n",
    "                 num_heads=[8, 8, 8],\n",
    "                 window_size=5,\n",
    "                 mlp_ratio=4.,\n",
    "                 qkv_bias=True,\n",
    "                 qk_scale=None,\n",
    "                 drop_rate=0.,\n",
    "                 attn_drop_rate=0.,\n",
    "                 drop_path_rate=0.1,\n",
    "                 norm_layer=nn.LayerNorm,\n",
    "                 ape=False,\n",
    "                 patch_norm=True,\n",
    "                 use_checkpoint=False,\n",
    "                 upscale=2,\n",
    "                 img_range=1.,\n",
    "                 upsampler='pixelshuffle',\n",
    "                 resi_connection='1conv',\n",
    "                 **kwargs):\n",
    "        super(Swin_Block, self).__init__()\n",
    "        num_in_ch = in_chans\n",
    "        num_out_ch = in_chans\n",
    "        num_feat = 64\n",
    "        self.img_range = img_range\n",
    "        if in_chans == 3:\n",
    "            rgb_mean = (0.4488, 0.4371, 0.4040)\n",
    "            self.mean = paddle.to_tensor(rgb_mean).reshape((1, 3, 1, 1))\n",
    "        else:\n",
    "            self.mean = paddle.zeros((1, 1, 1, 1))\n",
    "        self.upscale = upscale\n",
    "        self.upsampler = upsampler\n",
    "        self.window_size = window_size\n",
    "        self.time_dim = 256\n",
    "        #####################################################################################################\n",
    "        ################################### 1, shallow feature extraction ###################################\n",
    "        self.conv_first = nn.Conv2D(2, 64, 3, 1, 1)\n",
    "\n",
    "        #####################################################################################################\n",
    "        ################################### 2, deep feature extraction ######################################\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ape = ape\n",
    "        self.patch_norm = patch_norm\n",
    "        self.num_features = embed_dim\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        # split image into non-overlapping patches\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size,\n",
    "                                      patch_size=patch_size,\n",
    "                                      in_chans=embed_dim,\n",
    "                                      embed_dim=embed_dim,\n",
    "                                      norm_layer=norm_layer if self.patch_norm else None)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "        patches_resolution = self.patch_embed.patches_resolution\n",
    "        self.patches_resolution = patches_resolution\n",
    "\n",
    "        # merge non-overlapping patches into image\n",
    "        self.patch_unembed = PatchUnEmbed(img_size=img_size,\n",
    "                                          patch_size=patch_size,\n",
    "                                          in_chans=embed_dim,\n",
    "                                          embed_dim=embed_dim,\n",
    "                                          norm_layer=norm_layer if self.patch_norm else None)\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        # stochastic depth\n",
    "        dpr = [x.item() for x in paddle.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "\n",
    "        self.rstb = RSTB(\n",
    "                dim=embed_dim,\n",
    "                input_resolution=(patches_resolution[0], patches_resolution[1]),\n",
    "                depth=depths[1],\n",
    "                num_heads=num_heads[1],\n",
    "                window_size=window_size,\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:1]):sum(depths[:1 + 1])],  # no impact on SR results\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=None,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "                img_size=img_size,\n",
    "                patch_size=patch_size,\n",
    "                resi_connection=resi_connection)\n",
    "        \n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        \n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.initializer.Constant(0.0)(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.initializer.Constant(0.0)(m.bias)\n",
    "            nn.initializer.Constant(1.0)(m.weight)\n",
    "\n",
    "    def check_image_size(self, x):\n",
    "        _, _, h, w = x.shape\n",
    "        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n",
    "        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n",
    "        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n",
    "        return x\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (paddle.arange(0, channels, 2).astype(paddle.float32) / channels)\n",
    "        )\n",
    "        pos_enc_a = paddle.sin(t.tile([1, channels // 2]) * inv_freq)\n",
    "        pos_enc_b = paddle.cos(t.tile([1, channels // 2]) * inv_freq)\n",
    "        pos_enc = paddle.concat([pos_enc_a, pos_enc_b], axis=-1)\n",
    "        return pos_enc\n",
    "        \n",
    "    def forward(self, x0):\n",
    "        x_size = (x0.shape[2], x0.shape[3])\n",
    "        x = self.patch_embed(x0)\n",
    "\n",
    "        x = self.pos_drop(x)\n",
    "        x_sw=self.rstb(x, x_size)\n",
    "        # x = self.norm(x)  # B L C\n",
    "        # x = self.patch_unembed(x, x_size)\n",
    "        return x_sw+x0\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece46d91-39dc-4faf-bc4e-241b8585bb4b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#SDUnet\n",
    "class DoubleConv(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2D(in_channels, mid_channels, kernel_size=3, padding=1, bias_attr=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2D(mid_channels, out_channels, kernel_size=3, padding=1, bias_attr=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "class ResidualDenseBlock(nn.Layer):\n",
    "    \"\"\"Residual Dense Block.\n",
    "\n",
    "    Used in RRDB block in ESRGAN.\n",
    "\n",
    "    Args:\n",
    "        num_feat (int): Channel number of intermediate features.\n",
    "        num_grow_ch (int): Channels for each growth.\n",
    "        bias:\n",
    "    \"\"\"\n",
    "    def __init__(self,num_feat=64,out_ch=64,num_grow_ch=32,residual=True):\n",
    "        super(ResidualDenseBlock,self).__init__()\n",
    "        self.conv1 = nn.Conv2D(num_feat,num_grow_ch,3,1,1,bias_attr=residual)\n",
    "        self.conv2 = nn.Conv2D(num_feat + num_grow_ch, num_grow_ch, 3, 1, 1,bias_attr=residual)\n",
    "        self.conv3 = nn.Conv2D(num_feat + 2 * num_grow_ch, num_grow_ch, 3, 1, 1,bias_attr=residual)\n",
    "        self.conv4 = nn.Conv2D(num_feat + 3 * num_grow_ch, num_grow_ch, 3, 1, 1,bias_attr=residual)\n",
    "        self.conv5 = nn.Conv2D(num_feat + 4 * num_grow_ch, out_ch, 3, 1, 1,bias_attr=residual)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope=0.2)\n",
    "        # initialization\n",
    "        #default_init_weights([self.conv1, self.conv2, self.conv3, self.conv4, self.conv5], 0.1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(paddle.concat((x,x1),1)))  #这里指通道数的叠加，所以每次叠加结束，通道数都增加num_grow_ch\n",
    "        x3 = self.lrelu(self.conv3(paddle.concat((x, x1, x2), 1)))\n",
    "        x4 = self.lrelu(self.conv4(paddle.concat((x, x1, x2, x3), 1)))\n",
    "        x5 = self.conv5(paddle.concat((x,x1,x2,x3,x4),1))\n",
    "        return x5            \n",
    "\n",
    "class Down(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2D(2),\n",
    "            ResidualDenseBlock(in_channels, in_channels, residual=True),\n",
    "            ResidualDenseBlock(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.Silu(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].tile([1, 1, x.shape[-2], x.shape[-1]])\n",
    "        return x + emb\n",
    "\n",
    "class Up(nn.Layer):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            ResidualDenseBlock(in_channels, in_channels, residual=True),\n",
    "            ResidualDenseBlock(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.Silu(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = paddle.concat([skip_x, x], axis=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].tile([1, 1, x.shape[-2], x.shape[-1]])\n",
    "        return x + emb\n",
    "\n",
    "class UNet(nn.Layer):\n",
    "    def __init__(self, c_in=2, c_out=1, time_dim=256, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim=256\n",
    "        self.time_dim = time_dim\n",
    "        self.cov_before= nn.Sequential(\n",
    "            nn.Conv2D(c_in, 64, kernel_size=3, padding=1, bias_attr=False),\n",
    "            nn.GroupNorm(1, 64),\n",
    "            nn.GELU(),\n",
    "        )\n",
    "        self.swin_64=Swin_Block(in_chans=64,embed_dim=64,img_size=120)\n",
    "        self.swin_128=Swin_Block(in_chans=128,embed_dim=128,img_size=60)\n",
    "        self.swin_256=Swin_Block(in_chans=256,embed_dim=256,img_size=30)\n",
    "        # self.swin_512=Swin_Block(in_chans=512,embed_dim=512,img_size=15)\n",
    "\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 256)\n",
    "\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.up3 = Up(128, 64)\n",
    "\n",
    "        self.bot1 = ResidualDenseBlock(256, 512)\n",
    "        self.bot2 = ResidualDenseBlock(512, 512)\n",
    "        self.bot3 = ResidualDenseBlock(512, 256)\n",
    "\n",
    "        self.cov_last= nn.Sequential(\n",
    "            nn.Conv2D(64, 1, kernel_size=3, padding=1, bias_attr=False),\n",
    "            nn.GroupNorm(1, 1),\n",
    "            # nn.GELU(),\n",
    "        )\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (paddle.arange(0, channels, 2).astype(paddle.float32) / channels)\n",
    "        )\n",
    "        pos_enc_a = paddle.sin(t.tile([1, channels // 2]) * inv_freq)\n",
    "        pos_enc_b = paddle.cos(t.tile([1, channels // 2]) * inv_freq)\n",
    "        pos_enc = paddle.concat([pos_enc_a, pos_enc_b], axis=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self,LR_t,LR,t):\n",
    "        t = t.unsqueeze(-1).astype(paddle.float32)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        x=paddle.concat([LR_t, LR], axis=1)\n",
    "        \n",
    "        x1=self.cov_before(x)\n",
    "        x1=self.swin_64(x1)\n",
    "        x2=self.down1(x1,t)\n",
    "        \n",
    "        x2=self.swin_128(x2)\n",
    "        x3=self.down2(x2,t)\n",
    "        \n",
    "        x3=self.swin_256(x3)\n",
    "        x4=self.down3(x3,t)\n",
    "\n",
    "        x4=self.swin_256(x4)\n",
    "        x4=self.bot1(x4)\n",
    "        x4=self.bot2(x4)\n",
    "        x4=self.bot3(x4)\n",
    "        x4=self.swin_256(x4)\n",
    "\n",
    "        x=self.up1(x4, x3, t)\n",
    "        x=self.swin_128(x)\n",
    "\n",
    "        x=self.up2(x, x2, t)\n",
    "        x=self.swin_64(x)\n",
    "\n",
    "        x=self.up3(x, x1, t)\n",
    "        x=self.swin_64(x)\n",
    "        \n",
    "        x=self.cov_last(x)\n",
    "        return x\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fab9dc-9de3-46cc-935f-a977abea93d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"\"\"RRDPM\"\"\"\n",
    "import os\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "from paddle import optimizer\n",
    "import logging\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=50, img_size=120, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.k_k=30.0\n",
    "\n",
    "        self.N_N=self.prepare_noise_schedule()[:, None, None, None]\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        \n",
    "        N=paddle.zeros(self.noise_steps+1)\n",
    "        for t in range(1,self.noise_steps+1):\n",
    "            N[t]=(math.sin((t/self.noise_steps)*math.pi-math.pi/2)+1)/2\n",
    "        N[self.noise_steps]=1\n",
    "        N[0]=0.00001\n",
    "        return N\n",
    "\n",
    "\n",
    "    def noise_images(self, HR,LR, t):\n",
    "        e0=LR-HR\n",
    "        mu= HR+self.N_N[t]*e0\n",
    "        Ɛ = paddle.randn(shape=HR.shape)\n",
    "        return mu + self.k_k*paddle.sqrt(self.N_N[t]) * Ɛ/255,Ɛ\n",
    "        \n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return paddle.randint(low=1, high=45+1, shape=(n,))\n",
    "\n",
    "    def sample(self,HR,LR, model, n):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        t = paddle.to_tensor([45] * HR.shape[0]).astype(\"int64\")\n",
    "        x,Ɛ = self.noise_images(HR,LR, t)\n",
    "        model.eval()\n",
    "        with paddle.no_grad():\n",
    "            for i in tqdm(reversed(range(1, 45+1)), position=0):  #i=[1,30]\n",
    "                t = paddle.to_tensor([i] * x.shape[0]).astype(\"int64\")\n",
    "                predect_Ɛ = model(x,LR,t)\n",
    "                \n",
    "                alpha_t=self.N_N[i]-self.N_N[i-1]\n",
    "                Ɛ_1 = paddle.randn(shape=predect_Ɛ.shape)\n",
    "\n",
    "                x0=(x-self.N_N[t]*LR-self.k_k*paddle.sqrt(self.N_N[t]) * predect_Ɛ/255)/(1-self.N_N[t])\n",
    "                x=(self.N_N[t-1]/self.N_N[t])*x+alpha_t/self.N_N[t]*x0+self.k_k*paddle.sqrt(self.N_N[i-1]/self.N_N[i]*alpha_t)*Ɛ_1/255\n",
    "        model.train()\n",
    "        return x\n",
    "\n",
    "\n",
    "# model training\n",
    "def train(args):\n",
    "    device = args.device\n",
    "    # Model initialization\n",
    "    model = UNet()\n",
    "    model.train()\n",
    "    # Optimizer initialization\n",
    "    opt = optimizer.Adam(learning_rate=args.lr,beta1=0.9,beta2=0.99, parameters=model.parameters())\n",
    "    # Load the model and optimizer parameters\n",
    "    # model.set_state_dict(P.load('model/noisepre/noisepre_900.pdparams'))\n",
    "    # opt.set_state_dict(P.load('model/noisepre/noisepre_900.pdopt'))\n",
    "    mse = nn.MSELoss()\n",
    "    content_criterion = nn.L1Loss()\n",
    "    # Initialization of the diffusion model\n",
    "    diffusion = Diffusion(img_size=args.image_size, device=device)\n",
    "\n",
    "    for epoch in range(0,args.epochs):\n",
    "        logging.info(f\"Starting epoch {epoch}:\")\n",
    "        dataset = HRLRDataset()\n",
    "        data_loader = DataLoader(dataset, batch_size=args.batch_size, shuffle=True)  \n",
    "        pbar = tqdm(data_loader)\n",
    "        for i, images in enumerate(pbar):\n",
    "            HR=images[0]\n",
    "            LR=images[1]\n",
    "            t = diffusion.sample_timesteps(HR.shape[0])\n",
    "            x_t,Ɛ = diffusion.noise_images(HR,LR, t)\n",
    "            # Noise prediction\n",
    "            predect_Ɛ = model(x_t,LR, t)\n",
    "            loss = mse(predect_Ɛ, Ɛ)\n",
    "\n",
    "            opt.clear_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            pbar.set_postfix(MSE=loss.item(),epochh=epoch)\n",
    "\n",
    "        # Save the model parameters every 50 training rounds.\n",
    "        if epoch % 50 == 0:\n",
    "            paddle.save(model.state_dict(), f\"model-noise/noisepre/noisepre_{epoch}.pdparams\")\n",
    "            paddle.save(opt.state_dict(),f\"model-noise/noisepre/noisepre_{epoch}.pdopt\")\n",
    "   \n",
    "def launch():\n",
    "    import argparse\n",
    "\n",
    "    # parameter setting\n",
    "    class ARGS:\n",
    "        def __init__(self):\n",
    "            self.epochs = 1001\n",
    "            self.batch_size = 8\n",
    "            self.image_size = 120\n",
    "            self.dataset_path = r\"car-pair\"\n",
    "            self.device = \"cuda\"\n",
    "            self.lr = 1e-5\n",
    "\n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    launch()\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
